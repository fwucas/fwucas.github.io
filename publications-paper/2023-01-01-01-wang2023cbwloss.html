

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>CbwLoss: Constrained Bidirectional Weighted Loss for Self-Supervised Learning of Depth and Pose - About</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="About">
<meta property="og:title" content="CbwLoss: Constrained Bidirectional Weighted Loss for Self-Supervised Learning of Depth and Pose">


  <link rel="canonical" href="https://fwucas.github.io/publications-paper/2023-01-01-01-wang2023cbwloss">
  <meta property="og:url" content="https://fwucas.github.io/publications-paper/2023-01-01-01-wang2023cbwloss">



  <meta property="og:description" content="">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2023-01-01T00:00:00-08:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Fei Wang",
      "url" : "https://fwucas.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://fwucas.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="About Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://fwucas.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://fwucas.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://fwucas.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://fwucas.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://fwucas.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://fwucas.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://fwucas.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://fwucas.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://fwucas.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://fwucas.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://fwucas.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://fwucas.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://fwucas.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://fwucas.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://fwucas.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://fwucas.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://fwucas.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://fwucas.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://fwucas.github.io/assets/css/academicons.css"/>


<!-- Support for MatJax -->
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://fwucas.github.io/">About</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://fwucas.github.io/publications-paper/">Published Papers</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://fwucas.github.io/publications-patent/">Published Patents</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://fwucas.github.io/experience/">Experience</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://fwucas.github.io/honors-awards/">Honors & Awards</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://fwucas.github.io/thesis/">Thesis</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://fwucas.github.io/images//member/feiwang/feiwang.png " class="author__avatar" alt="Fei Wang, Ph.D.">
    
  </div>

  <div class="author__content">
    <h3 class="author__name"><strong><span style="color: black;">Fei Wang, Ph.D.</span></strong></h3>
    
    <p class="author__bio"><strong><span style="color: black;">Computer Vision, Depth Estimation, Robot, Autonomous Vehicles</span></strong></p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      <!-- Font Awesome icons / Biographic information  -->
      
        <li class="author__desktop"><i class="fa-solid fa-location-dot icon-pad-right" aria-hidden="true"></i>Shenzhen, Guangdong, China</li>
      
      
        <li class="author__desktop"><a href='https://www.ucas.ac.cn/'><i class="fa fa-solid fa-building-columns icon-pad-right" aria-hidden="true"></i>: UCAS</a></li>
      
      
      
        <li><a href="mailto:fei.wang2@siat.ac.cn"><i class="fas fa-fw fa-envelope icon-pad-right" aria-hidden="true"></i>:  fei.wang2@siat.ac.cn</a></li>
      

      <!-- Font Awesome and Academicons icons / Academic websites -->
            
      
      
      
      
        <li><a href="https://orcid.org/0000-0002-4497-2675"><i class="ai ai-orcid ai-fw icon-pad-right"></i>ORCID</a></li>
      
                              
      

      <!-- Font Awesome icons / Repositories and software development -->
      
            
            
      
        <li><a href="https://github.com/fwucas"><i class="fab fa-fw fa-github icon-pad-right" aria-hidden="true"></i>Github</a></li>
      
            
            

      <!-- Font Awesome icons / Social media -->
      
      
            
      
                  
                  
      
            
            
            
      
            
                  
            
      
            
            
      
              
      
                      
      
      
            
    </ul>
  </div>
</div>

  
  </div>


  <div class="archive">
    
      <h2 class="page__title">CbwLoss: Constrained Bidirectional Weighted Loss for Self-Supervised Learning of Depth and Pose</h2>
    
    
<p><br /></p>

<center>Fei Wang<sup>1,2,3</sup>,&nbsp;  Jun Cheng<sup>1,3</sup>, &nbsp; Penglei Liu <sup>1,2,3</sup></center>

<center>1 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; 2 University of Chinese Academy of Sciences; &nbsp;  <br /> 3 The Chinese University of Hong Kong </center>

<center>Fei Wang<sup>1,2,3</sup>,&nbsp;  Jun Cheng<sup>1,3</sup>, &nbsp; Penglei Liu <sup>1,2,3</sup></center>

<p>         Abstract— Photometric differences are widely used as supervision signals to train neural networks for estimating depth and camera pose from unlabeled monocular videos. However, this approach is detrimental for model optimization because occlusions and moving objects in a scene violate the underlying static scenario assumption. In addition, pixels in textureless regions or less discriminative pixels hinder model training. To address these problems, in this paper, we deal with moving objects and occlusions by utilizing the differences between the flow fields, and the differences between the depth structure generated by affine transformation and view synthesis, respectively. Secondly, we mitigate the effect of textureless regions on model optimization by measuring the differences between features with more semantic and contextual information without requiring additional networks. In addition, although the bidirectionality component is used in each sub-objective function, a pair of images is reasoned about only once, which helps reduce overhead. Extensive experiments and visual analysis demonstrate the effectiveness of the proposed method, which outperforms existing state-of-the-art self-supervised methods under the same conditions and without introducing additional auxiliary information.</p>

<p><br /></p>

<p><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/overview.png" /></p>

<center> Diagram of the general framework</center>

<h3 id="results">Results</h3>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/kitti_result_vis.png" /></center>
<center> Qualitative comparison of example results of our proposed self-supervised monocular depth estimation method with those of previous state-of-the-art methods as estimated on the KITTI dataset. The ground truth maps were obtained from sparse laser data for visualization only. The brighter an area in a depth map is, the closer it is to the camera.</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/kitti_result_vis_error_map.png" /></center>
<center> Comparison of error maps between different methods on the KITTI dataset</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/ddad_result_vis.png" /></center>
<center> Qualitative comparison of example results of our proposed self-supervised monocular depth estimation method with those of previous state-of-the-art methods as estimated on the DDAD dataset.</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/ddad_result_vis_error_map.png" /></center>
<center> Comparison of error maps between different methods on the DDAD dataset.</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/camera_flow_occusion_mask.png" /></center>
<center>  Visualization analysis of the camera flow occlusion masks. From top to bottom are the original images, the forward camera flows generated from the projection transformation of the reference images to the target images, the backward camera flows synthesized using the differentiable bilinear sampling mechanism, the bidirectional camera flow occlusion masks obtained by checking the consistency of the above two flows, and the estimated depth maps, respectively.</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/adative_weight_vis.png" /></center>
<center>  Visualization analysis of the adaptive weights. From top to bottom are the original images, the depth maps obtained through projection transformation, the depth maps obtained through view synthesis, the adaptive weights obtained by comparing the above two depth maps, and the estimated depth maps, respectively.</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/feat_vis.png" /></center>
<center> Visualization analysis of the learned feature maps. Here, we select only the principal feature map for visualization utilizing principal component analysis. From top to bottom are the original images, the feature maps without the bidirectional feature perception loss, the depth maps without the bidirectional
feature perception loss, the feature maps with the bidirectional feature perception loss, and the depth maps with the bidirectional feature perception loss, respectively</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/272_kitti_result.png" /></center>
<center> Comparison of performance for monocular depth estimation on the 282 images selected from 697 images in accordance with eigen’s testing split. (272 images with moving objects and 10 images in which most areas are textureless)</center>

<center><img src="../images/paper_images/2023-01-01-01-wang2023cbwloss/ddad_results.png" /></center>
<center> Comparison of performance for monocular depth estimation on the DDAD dataset</center>

<h3 id="downloads">Downloads</h3>

<p><strong>Fei Wang</strong>, Jun Cheng and Penglei Liu. CbwLoss: Constrained Bidirectional Weighted Loss for Self-Supervised Learning of Depth and Pose [J]. <strong>IEEE Transactions on Intelligent Transportation Systems (TITS)</strong>, vol. 24, no. 6, pp. 5803-5821, June 2023.</p>

<p><a href="/files/paper/CbwLoss_Constrained_Bidirectional_Weighted_Loss_for_Self-Supervised_Learning_of_Depth_and_Pose_2023.pdf"><img src="./assets/pdf.png" height="60" width="60" /> Paper</a></p>

  </div>
</div>


    <div class="page__footer">
      <footer>
        <!DOCTYPE html>
<html>
<head>
    <title>Your Page Title</title>
    <!-- 在这里插入百度统计代码 -->
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b28ad6c0167645a6c36d53f6f2167ebc";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?your_baidu_tongji_id";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
</head>
<body>
</body>
</html>

        
 
<div class="no-wrap">
<strong>Collaboration:</strong> &emsp;<a href='https://english.ucas.ac.cn/'><img src="/images/logo/ucas_logo.png" width="100" height="100" alt='UCAS'> University of Chinese Academy of Sciences</a>&emsp; 
<a href="https://en.xtu.edu.cn/"><img src="/images/logo/xtu_logo.png" width="30" height="30" alt='XTU'> Xiangtan University</a> 
&emsp; <a href="https://english.sic.cas.cn/"><img src="/images/logo/siccas.png" width="50" height="40" alt='sic.cas'> Shanghai Institute of Ceramics，Chinese Academy of Sciences</a> 
</div>
<div class="page__footer-copyright"> &copy; 2024 Fei Wang.</div>

      </footer>
    </div>

    <script src="https://fwucas.github.io/assets/js/main.min.js"></script>









  </body>
</html>

